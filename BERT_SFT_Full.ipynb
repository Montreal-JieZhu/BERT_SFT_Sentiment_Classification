{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "e89a07cd",
   "metadata": {},
   "source": [
    "### In this section, I Fine-tuned a pretrained BERT model on the IMDB dataset for sentiment analysis, evaluate it with accuracy / F1, and run inference using the Hugging Face Transformers library."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "3eba6788",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "W1019 16:44:52.873000 440 Lib\\site-packages\\torch\\distributed\\elastic\\multiprocessing\\redirects.py:29] NOTE: Redirects are currently not supported in Windows or MacOs.\n"
     ]
    }
   ],
   "source": [
    "# pip install transformers datasets evaluate accelerate scikit-learn\n",
    "import torch\n",
    "from datasets import load_dataset\n",
    "from transformers import (AutoTokenizer, AutoModelForSequenceClassification,\n",
    "                          DataCollatorWithPadding, Trainer, TrainingArguments)\n",
    "import evaluate\n",
    "import numpy as np"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "1c1284b1",
   "metadata": {},
   "outputs": [
    {
     "ename": "NameError",
     "evalue": "name 'torch' is not defined",
     "output_type": "error",
     "traceback": [
      "\u001b[1;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[1;31mNameError\u001b[0m                                 Traceback (most recent call last)",
      "Cell \u001b[1;32mIn[1], line 2\u001b[0m\n\u001b[0;32m      1\u001b[0m \u001b[38;5;66;03m# --- GPU / Mixed-Precision setup ---\u001b[39;00m\n\u001b[1;32m----> 2\u001b[0m use_cuda \u001b[38;5;241m=\u001b[39m \u001b[43mtorch\u001b[49m\u001b[38;5;241m.\u001b[39mcuda\u001b[38;5;241m.\u001b[39mis_available()\n\u001b[0;32m      3\u001b[0m use_bf16 \u001b[38;5;241m=\u001b[39m use_cuda \u001b[38;5;129;01mand\u001b[39;00m torch\u001b[38;5;241m.\u001b[39mcuda\u001b[38;5;241m.\u001b[39mis_bf16_supported()     \u001b[38;5;66;03m# Ampere+ GPUs\u001b[39;00m\n\u001b[0;32m      4\u001b[0m use_fp16 \u001b[38;5;241m=\u001b[39m use_cuda \u001b[38;5;129;01mand\u001b[39;00m \u001b[38;5;129;01mnot\u001b[39;00m use_bf16                       \u001b[38;5;66;03m# fallback to fp16 if bf16 not available\u001b[39;00m\n",
      "\u001b[1;31mNameError\u001b[0m: name 'torch' is not defined"
     ]
    }
   ],
   "source": [
    "# --- GPU / Mixed-Precision setup ---\n",
    "use_cuda = torch.cuda.is_available()\n",
    "use_bf16 = use_cuda and torch.cuda.is_bf16_supported()     # Ampere+ GPUs\n",
    "use_fp16 = use_cuda and not use_bf16                       # fallback to fp16 if bf16 not available\n",
    "print(f\"CUDA available: {use_cuda} | bf16: {use_bf16} | fp16: {use_fp16} | GPUs: {torch.cuda.device_count()}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "0ff2f939",
   "metadata": {},
   "outputs": [],
   "source": [
    "MODEL_NAME = \"bert-base-uncased\"\n",
    "NUM_LABELS = 2  # binary classification\n",
    "\n",
    "# 1) Data: IMDB (binary sentiment)\n",
    "dataset = load_dataset(\"imdb\")  # splits: train/test"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "e5f51c6e",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "{'train': ['text', 'label'],\n",
       " 'test': ['text', 'label'],\n",
       " 'unsupervised': ['text', 'label']}"
      ]
     },
     "execution_count": 3,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "dataset.column_names"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "900f7e5f",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "{'text': ['I love sci-fi and am willing to put up with a lot. Sci-fi movies/TV are usually underfunded, under-appreciated and misunderstood. I tried to like this, I really did, but it is to good TV sci-fi as Babylon 5 is to Star Trek (the original). Silly prosthetics, cheap cardboard sets, stilted dialogues, CG that doesn\\'t match the background, and painfully one-dimensional characters cannot be overcome with a \\'sci-fi\\' setting. (I\\'m sure there are those of you out there who think Babylon 5 is good sci-fi TV. It\\'s not. It\\'s clichéd and uninspiring.) While US viewers might like emotion and character development, sci-fi is a genre that does not take itself seriously (cf. Star Trek). It may treat important issues, yet not as a serious philosophy. It\\'s really difficult to care about the characters here as they are not simply foolish, just missing a spark of life. Their actions and reactions are wooden and predictable, often painful to watch. The makers of Earth KNOW it\\'s rubbish as they have to always say \"Gene Roddenberry\\'s Earth...\" otherwise people would not continue watching. Roddenberry\\'s ashes must be turning in their orbit as this dull, cheap, poorly edited (watching it without advert breaks really brings this home) trudging Trabant of a show lumbers into space. Spoiler. So, kill off a main character. And then bring him back as another actor. Jeeez! Dallas all over again.',\n",
       "  \"Worth the entertainment value of a rental, especially if you like action movies. This one features the usual car chases, fights with the great Van Damme kick style, shooting battles with the 40 shell load shotgun, and even terrorist style bombs. All of this is entertaining and competently handled but there is nothing that really blows you away if you've seen your share before.<br /><br />The plot is made interesting by the inclusion of a rabbit, which is clever but hardly profound. Many of the characters are heavily stereotyped -- the angry veterans, the terrified illegal aliens, the crooked cops, the indifferent feds, the bitchy tough lady station head, the crooked politician, the fat federale who looks like he was typecast as the Mexican in a Hollywood movie from the 1940s. All passably acted but again nothing special.<br /><br />I thought the main villains were pretty well done and fairly well acted. By the end of the movie you certainly knew who the good guys were and weren't. There was an emotional lift as the really bad ones got their just deserts. Very simplistic, but then you weren't expecting Hamlet, right? The only thing I found really annoying was the constant cuts to VDs daughter during the last fight scene.<br /><br />Not bad. Not good. Passable 4.\",\n",
       "  \"its a totally average film with a few semi-alright action sequences that make the plot seem a little better and remind the viewer of the classic van dam films. parts of the plot don't make sense and seem to be added in to use up time. the end plot is that of a very basic type that doesn't leave the viewer guessing and any twists are obvious from the beginning. the end scene with the flask backs don't make sense as they are added in and seem to have little relevance to the history of van dam's character. not really worth watching again, bit disappointed in the end production, even though it is apparent it was shot on a low budget certain shots and sections in the film are of poor directed quality\"],\n",
       " 'label': [0, 0, 0]}"
      ]
     },
     "execution_count": 4,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "dataset[\"test\"][:3]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "57084852",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "{'text': ['I saw this film opening weekend in Australia, anticipating with an excellent cast of Ledger, Edgerton, Bloom, Watts and Rush that the definitive story of Ned Kelly would unfold before me. Unfortunately, despite an outstanding performance by Heath Ledger in the lead role, the plot was paper thin....which doesn\\'t inspire me to read \"Our Sunshine\". There were some other plus points, the support acting from Edgerton in particular, assured direction from Jordan (confirming his talent on show in Buffalo Soldiers as well), and production design that gave a real feel of harshness to the Australian bush, much as the Irish immigrants of the early 19th century must have seen it. But I can\\'t help feeling that another opportunity has been missed to tell the real story of an Australian folk hero (or was he?)....in what I suspect is a concession to Hollywood and selling the picture in the US. Oh well, at least Jordan and the producers didn\\'t agree to lose the beards just to please Universal...<br /><br />Guess I will just have to content myself with Peter Carey\\'s excellent \"Secret History of the Kelly Gang\". 4/10',\n",
       "  'I saw this at the premiere in Melbourne<br /><br />It is shallow, two-dimensional, unaffecting and, hard to believe given the subject matter, boring. The actors are passable, but they didn\\'t have much to work with given the very plodding and unimpressive script. For those who might have worried that Ned Kelly would be over-intellectualised, you can take comfort in the fact that this telling of the story is utterly without any literary depth at all, told entirely on the surface and full of central casting standards. However, it doesn\\'t work as a popcorn film either. Its pacing is too off-kilter and its craft is too lacking to satisfy even on the level of a mundane actioner.<br /><br />I very much doubt Gregor Jordan could sit back and say to himself \"this is the best I could have done with the material\".<br /><br />Ned Kelly is a fascinating figure, and equally so is the national response to him. Possibly folk genius, possibly class warrior, possibly psychopath and probably all these things, he has dominated Australian true mythology for over 120 years. Once again, his story has failed miserably on the big screen.<br /><br />Such is life.',\n",
       "  \"Ned Kelly (Ledger), the infamous Australian outlaw and legend. Sort of like Robin Hood, with a mix of Billy the Kid, Australians love the legend of how he stood up against the English aristocratic oppression, and united the lower classes to change Australia forever. The fact that the lower classes of the time were around 70% immigrant criminals seems to be casually skimmed around by this film. Indeed, quite a few so called `facts' in this film are, on reflection, a tad dubious.<br /><br />I suppose the suspicions should have been aroused when, in the opening credits, it was claimed that this film is based upon the book, `Our Sunshine'. If ever a romanticized version of truth could be seen in a name for a book, there it was. This wasn't going to be a historical epic, but just an adaptation of one of many dubious legends of Ned Kelly, albeit a harsh and sporadically brutal version.<br /><br />Unfortunately, Ned Kelly is nothing more than an overblown Hallmark channel `real life historical drama' wannabe! The story plods along at an alarming rate (alarming because never has a film plodded so slowly!) The feeling of numbness after the two hours of pure drivel brought back memories of Costner's awful Wyatt Earp all those years ago. Simply put, nothing happens in the film, but it takes a long time getting to that nothing. This would possibly have been a tad more bearable if the performances were good (because the direction sure as heck wasn't). However, unless you are looking to play a game of spot the worst Oirish accent, then you're gonna be disappointed. Between that, the game of `Who has the stupidest beard?', `Spot the obvious backstabber!' (clue, they are all ginger for some reason), and `Nature in Australia.including lions', it is an experience similar to flicking through Hallmark, The History Channel, Discovery Channel, and Neighbours whilst suffering a huge hangover. Yup, nature pops up a lot, as to fill even more time (possibly an attempt to look arty), the film keeps showing pointless wildlife shots, and once all the native species are shown, here's a circus to allow for a camel and a lion (which is used during one fight to try to make us actually feel more sorry for the lion than the massacred people).<br /><br />This is a turgid, emotionless piece of historical fluff which should have gone straight to TV. There isn't even one good word I can say about this film. Even the usually fantastic Rush seems embarrassed to be here. When one of the characters comments that there is only 2 bullets left for him and his pal, I myself was wishing I had a gun to blow any memory of this film out of my head!\"],\n",
       " 'label': [0, 0, 0]}"
      ]
     },
     "execution_count": 5,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "dataset[\"train\"][50:53]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "20f42bd0",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "{'text': ['This is just a precious little diamond. The play, the script are excellent. I cant compare this movie with anything else, maybe except the movie \"Leon\" wonderfully played by Jean Reno and Natalie Portman. But... What can I say about this one? This is the best movie Anne Parillaud has ever played in (See please \"Frankie Starlight\", she\\'s speaking English there) to see what I mean. The story of young punk girl Nikita, taken into the depraved world of the secret government forces has been exceptionally over used by Americans. Never mind the \"Point of no return\" and especially the \"La femme Nikita\" TV series. They cannot compare the original believe me! Trash these videos. Buy this one, do not rent it, BUY it. BTW beware of the subtitles of the LA company which \"translate\" the US release. What a disgrace! If you cant understand French, get a dubbed version. But you\\'ll regret later :)',\n",
       "  'When I say this is my favourite film of all time, that comment is not to be taken lightly. I probably watch far too many films than is healthy for me, and have loved quite a few of them. I first saw \"La Femme Nikita\" nearly ten years ago, and it still manages to be my absolute favourite. Why?<br /><br />This is more than an incredibly stylish and sexy thriller. Luc Besson\\'s great flair for impeccable direction, fashion, and appropriate usage of music makes this a very watchable film. But it is Anne Parillaud\\'s perfect rendering of a complex character who transforms from a heartless killer into a compassionate, vibrant young woman that makes this film beautiful. I can\\'t keep my eyes off of her when she is on screen.<br /><br />I have seen several of Luc Besson\\'s films including \"Subway\", \"The Professional\", and the irritating \"Fifth Element\", and \"Nikita\" is without a doubt, far superior to any of these. Although this film has tragic elements, it is ultimately extremely hopeful. It is the story of a person who is cruel and merciless, who ultimately comes to realize her own humanity and her own personal power. That, to me is extremely inspiring. If there is hope for Nikita, there is hope for all of us.',\n",
       "  'I saw this movie because I am a huge fan of the TV series of the same name starring Roy Dupuis and Pet Wilson. The movie was really good and I saw how the TV show is based on the movie. A few episodes of the TV series came directly from the movie and their similarity was amazing. To keep things short, any fan of the movie has to watch the series and any fan of the series must see the original Nikita.'],\n",
       " 'label': [-1, -1, -1]}"
      ]
     },
     "execution_count": 6,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "dataset[\"unsupervised\"][:3]"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "0557ac66",
   "metadata": {},
   "source": [
    "##### SFT needs labeled data. So removing unlabeled data."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "a59c6f06",
   "metadata": {},
   "outputs": [],
   "source": [
    "del dataset[\"unsupervised\"]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "id": "2eff7b17",
   "metadata": {},
   "outputs": [],
   "source": [
    "# 2) Tokenizer\n",
    "tokenizer = AutoTokenizer.from_pretrained(MODEL_NAME)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "id": "ac45b02f",
   "metadata": {},
   "outputs": [],
   "source": [
    "def preprocess(batch):\n",
    "    return tokenizer(batch[\"text\"], truncation=True)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "8157b323",
   "metadata": {},
   "source": [
    "##### Tokenizing the text based on its vocab so that machine can process it.\n",
    "##### Models like BERT or GPT don’t read English words the way humans do. They can only process numbers — so we must first convert text into numbers that the model can understand. That’s what tokenization and token IDs do."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "id": "f24cb83b",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "4082dbd1cf8e4a549962c63b8992d05b",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Map:   0%|          | 0/25000 [00:00<?, ? examples/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "tokenized = dataset.map(preprocess, batched=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "id": "75fe9b46",
   "metadata": {},
   "outputs": [],
   "source": [
    "data_collator = DataCollatorWithPadding(tokenizer=tokenizer)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "id": "a9f719d8",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Some weights of BertForSequenceClassification were not initialized from the model checkpoint at bert-base-uncased and are newly initialized: ['classifier.bias', 'classifier.weight']\n",
      "You should probably TRAIN this model on a down-stream task to be able to use it for predictions and inference.\n"
     ]
    }
   ],
   "source": [
    "# 3) Model\n",
    "model = AutoModelForSequenceClassification.from_pretrained(MODEL_NAME, num_labels=NUM_LABELS)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "d750dd02",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Optional: reduce memory on large batches\n",
    "if use_cuda:\n",
    "    model.gradient_checkpointing_enable()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "id": "af600ddb",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "94e8f6914b734029a7f1c4b194e591ae",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Downloading builder script: 0.00B [00:00, ?B/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "1a9b3d13b906424d8477224f270e0b30",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Downloading builder script: 0.00B [00:00, ?B/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "# 4) Metrics\n",
    "accuracy = evaluate.load(\"accuracy\")\n",
    "f1 = evaluate.load(\"f1\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "id": "b0b51f83",
   "metadata": {},
   "outputs": [],
   "source": [
    "def compute_metrics(eval_pred):\n",
    "    logits, labels = eval_pred\n",
    "    preds = np.argmax(logits, axis=-1)\n",
    "    return {\n",
    "        \"accuracy\": accuracy.compute(predictions=preds, references=labels)[\"accuracy\"],\n",
    "        \"f1\": f1.compute(predictions=preds, references=labels, average=\"weighted\")[\"f1\"]\n",
    "    }"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "20aae2a3",
   "metadata": {},
   "outputs": [],
   "source": [
    "# 5) Training config (GPU-aware)\n",
    "args = TrainingArguments(\n",
    "    output_dir=\"bert-imdb\",\n",
    "    eval_strategy=\"epoch\",\n",
    "    save_strategy=\"epoch\",\n",
    "    learning_rate=2e-5,\n",
    "    per_device_train_batch_size=16,   # adjust up if you have more GPU memory\n",
    "    per_device_eval_batch_size=32,\n",
    "    num_train_epochs=3,\n",
    "    weight_decay=0.01,\n",
    "    warmup_ratio=0.1,\n",
    "    logging_steps=50,\n",
    "    load_best_model_at_end=True,\n",
    "    metric_for_best_model=\"f1\",\n",
    "    bf16=use_bf16,                    # mixed precision on Ampere+ (preferred)\n",
    "    fp16=use_fp16,                    # else use fp16\n",
    "    dataloader_pin_memory=True,\n",
    "    optim=\"adamw_torch\",              # fast fused AdamW in recent PyTorch\n",
    "    torch_compile=True if use_cuda and torch.__version__.startswith(\"2.\") else False,\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "3a9dc17b",
   "metadata": {},
   "outputs": [],
   "source": [
    "# 6) Trainer (Trainer uses GPU automatically if available)\n",
    "trainer = Trainer(\n",
    "    model=model,\n",
    "    args=args,\n",
    "    train_dataset=tokenized[\"train\"],\n",
    "    eval_dataset=tokenized[\"test\"],\n",
    "    processing_class=tokenizer,\n",
    "    data_collator=data_collator,\n",
    "    compute_metrics=compute_metrics,\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "36183f80",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "c:\\Users\\Montr\\AI_Projects\\.venv\\Lib\\site-packages\\torch\\utils\\data\\dataloader.py:666: UserWarning: 'pin_memory' argument is set as true but no accelerator is found, then device pinned memory won't be used.\n",
      "  warnings.warn(warn_msg)\n"
     ]
    }
   ],
   "source": [
    "trainer.train()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "efd5cded",
   "metadata": {},
   "outputs": [],
   "source": [
    "# 7) Save + quick inference\n",
    "trainer.save_model(\"bert-imdb/best\")\n",
    "tokenizer.save_pretrained(\"bert-imdb/best\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "80635781",
   "metadata": {},
   "outputs": [],
   "source": [
    "from transformers import pipeline\n",
    "# device_map=\"auto\" will place the model on GPU if available for inference\n",
    "clf = pipeline(\"text-classification\", model=\"bert-imdb/best\", tokenizer=\"bert-imdb/best\", device_map=\"auto\")\n",
    "print(clf(\"This movie was absolutely wonderful!\"))\n",
    "print(clf(\"Terrible plot and wooden acting.\"))"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "SFT_QLoRA_Llama3_1B",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.13"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
